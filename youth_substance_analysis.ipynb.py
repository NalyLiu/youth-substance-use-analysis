# -*- coding: utf-8 -*-
"""Final_Colab_Zhao_Liu_Zheng.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1adMjDDPBrtOhjwDuK8wUz5XvdSUgrlmK
"""

# Commented out IPython magic to ensure Python compatibility.
=# Import the 'drive' module from the 'google.colab' package to enable Google Drive integration.
# Then, mount Google Drive to the '/drive' directory within the Colab environment.
# The 'force_remount=True' parameter ensures that the Drive is remounted even if it was previously mounted.

from google.colab import drive
drive.mount('/drive', force_remount=True)

# Change the current working directory to the specified path within Google Drive.
# %cd '/drive/MyDrive/40 Final Project/Final Data'

"""# **3.Data Cleaning**"""

import pandas as pd
# 3.1.1 Load the dataset (CSV file:'youth_smoking_drug_data.csv')
fin = input("Enter the input file name: ")
df = pd.read_csv(fin)
df.head()

# 3.1.2 Display basic information about the dataset
df.shape

# 3.1.3 Display the structure and data types
df.info()

# 3.2 Check for for missing values
df.isnull().sum()

# 3.3 Detect and handle outliers
# Identify numerical columns
numerical_columns = df.select_dtypes(include=['float64', 'int64']).columns

# Initialize a dictionary to store the results
outlier_summary = {}

for col in numerical_columns:
    Q1 = df[col].quantile(0.25)
    Q3 = df[col].quantile(0.75)
    IQR = Q3 - Q1
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR

    # Check if any outliers exist in the column
    has_outliers = ((df[col] < lower_bound) | (df[col] > upper_bound)).any()
    outlier_summary[col] = has_outliers

# Display the results
print("Outlier Detection Summary:")
for column, has_outlier in outlier_summary.items():
    if has_outlier:
        print(f"{column}: Outliers Found")
    else:
        print(f"{column}: No Outliers")

# 3.4.1 Check for duplicate rows
df.duplicated().sum()

# 3.4.2 Detect Typo
# Define expected categories for validation
expected_categories = {
    'Age_Group': ['10-14', '15-19', '20-24', '25-29', '30-39', '40-49', '50-59', '60-69', '70-79', '80+'],
    'Gender': ['Both', 'Female', 'Male'],
    'Socioeconomic_Status': ['High', 'Middle', 'Low'],
    'School_Programs': ['Yes', 'No'],
    'Access_to_Counseling': ['Yes', 'No'],
    'Substance_Education': ['Yes', 'No']
}

# Check each column for unexpected values
for col in expected_categories:
    valid_values = expected_categories[col]
    actual_values = set(df[col].unique())
    unexpected_values = actual_values - set(valid_values)
    if unexpected_values:
        print(f"{col}: Unexpected values found: {unexpected_values}")
    else:
        print(f"{col}: All values are valid.")

"""# **4. Descriptive Analytics**"""

# 4.1 Calculate summary statistics
# Calculate Descriptive Statistics for Numerical Columns
print("\nDescriptive Statistics for Numerical Columns:")
print(df.describe())

# Visualization of 4.1 (Mean, Standard Deviation)
import matplotlib.pyplot as plt

# Extract mean and standard deviation
stats = df.drop(columns=['Year']).describe().T[['mean', 'std']]

# Plot bar chart
stats.plot(kind='bar', figsize=(12, 6))
plt.title('Mean and Standard Deviation of Numerical Columns')
plt.ylabel('Value')
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()

# Visualization of 4.1 (Minimum, Maximum)
# Extract min and max
stats = df.drop(columns=['Year']).describe().T[['min', 'max']]

# Plot bar chart for min and max
stats.plot(kind='bar', figsize=(12, 6))
plt.title('Range of Numerical Variables (Min and Max)')
plt.ylabel('Value')
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()

# Visualization of 4.1 (Percentiles 25%, 50%, 75%)
# Box plot for all numerical columns
numerical_columns = df.select_dtypes(include=['float64', 'int64']).drop(columns=['Year'])
# Box plot for all numerical columns
numerical_columns.boxplot(figsize=(12, 6))
plt.title('Box Plot of Numerical Columns (Excluding Year)')
plt.ylabel('Values')
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()

# 4.2 Explore and describe the distribution of numerical data using skewness and kurtosis
from scipy.stats import skew, kurtosis

# Select numerical columns
numerical_columns = df.select_dtypes(include=['float64', 'int64']).columns

# Explore skewness and kurtosis for each numerical column
print("Distribution Summary (Skewness and Kurtosis):")
for col in numerical_columns:
    col_skewness = skew(df[col].dropna())
    col_kurtosis = kurtosis(df[col].dropna())
    print(f"{col}: Skewness = {col_skewness:.2f}, Kurtosis = {col_kurtosis:.2f}")

# Visualization of 4.2
import matplotlib.pyplot as plt

# Plot histogram for each numerical column
for column in numerical_columns:
    plt.figure(figsize=(8, 5))
    df[column].hist(bins=20, color='skyblue', alpha=0.7)
    plt.title(f'Distribution of {column} (Skewness: {skew(df[column]):.2f})')
    plt.xlabel(column)
    plt.ylabel('Frequency')
    plt.tight_layout()
    plt.show()

# 4.4 Generate frequency distributions for categorical data
object_columns = df.select_dtypes(include=['object']).columns
for col in object_columns:
    print(f"\nFrequency Distribution for {col}:")
    print(df[col].value_counts())

# Visualize Numerical Distributions of 4.4
import matplotlib.pyplot as plt

# Histogram for Smoking Prevalence
df['Smoking_Prevalence'].hist(bins=20, color='skyblue', alpha=0.7)
plt.title("Distribution of Smoking Prevalence")
plt.xlabel("Smoking Prevalence")
plt.ylabel("Frequency")
plt.show()

# Histogram for Drug Experimentation
df['Drug_Experimentation'].hist(bins=20, color='orange', alpha=0.7)
plt.title("Distribution of Drug Experimentation")
plt.xlabel("Drug Experimentation")
plt.ylabel("Frequency")
plt.show()

# 4.5 Segment the data by relevant categories (e.g., by group, region) to identify trends
# Analysis
age_group_analysis = df.groupby('Age_Group')[['Smoking_Prevalence', 'Drug_Experimentation']].mean().reset_index()
gender_analysis = df.groupby('Gender')[['Smoking_Prevalence', 'Drug_Experimentation']].mean().reset_index()
socioecon_analysis = df.groupby('Socioeconomic_Status')[['Smoking_Prevalence', 'Drug_Experimentation']].mean().reset_index()
df['Peer_Influence_Level'] = df['Peer_Influence'].apply(lambda x: 'High' if x > 5 else 'Low')
peer_influence_analysis = df.groupby('Peer_Influence_Level')[['Smoking_Prevalence', 'Drug_Experimentation']].mean().reset_index()
family_median = df['Family_Background'].median()
df['Family_Support_Level'] = df['Family_Background'].apply(lambda x: 'High' if x > family_median else 'Low')
family_support_analysis = df.groupby('Family_Support_Level')[['Smoking_Prevalence', 'Drug_Experimentation']].mean().reset_index()
school_program_analysis = df.groupby('School_Programs')[['Smoking_Prevalence', 'Drug_Experimentation']].mean().reset_index()
education_analysis = df.groupby('Substance_Education')[['Smoking_Prevalence', 'Drug_Experimentation']].mean().reset_index()
cross_analysis = df.groupby(['Gender', 'Socioeconomic_Status'])[['Smoking_Prevalence', 'Drug_Experimentation']].mean().reset_index()

# Print results
print(age_group_analysis)
print(gender_analysis)
print(socioecon_analysis)
print(peer_influence_analysis)
print(family_support_analysis)
print(school_program_analysis)
print(education_analysis)
print(cross_analysis)

# Visualization of 4.5.1
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np

# Function to plot grouped bar charts
def plot_grouped_bar(data, x_col, y1_col, y2_col, xlabel, ylabel, title):
    x = np.arange(len(data[x_col]))
    width = 0.35

    plt.figure(figsize=(10, 6))
    plt.bar(x - width/2, data[y1_col], width, label='Smoking Prevalence')
    plt.bar(x + width/2, data[y2_col], width, label='Drug Experimentation')
    plt.xticks(x, data[x_col], rotation=45)
    plt.xlabel(xlabel)
    plt.ylabel(ylabel)
    plt.title(title)
    plt.legend()
    plt.tight_layout()
    plt.show()

# Visualization
plot_grouped_bar(age_group_analysis, 'Age_Group', 'Smoking_Prevalence', 'Drug_Experimentation',
                 'Age Group', 'Mean Percentage', 'Smoking and Drug Experimentation by Age Group')

plot_grouped_bar(gender_analysis, 'Gender', 'Smoking_Prevalence', 'Drug_Experimentation',
                 'Gender', 'Mean Percentage', 'Smoking and Drug Experimentation by Gender')

plot_grouped_bar(socioecon_analysis, 'Socioeconomic_Status', 'Smoking_Prevalence', 'Drug_Experimentation',
                 'Socioeconomic Status', 'Mean Percentage', 'Smoking and Drug Experimentation by Socioeconomic Status')

plot_grouped_bar(peer_influence_analysis, 'Peer_Influence_Level', 'Smoking_Prevalence', 'Drug_Experimentation',
                 'Peer Influence Level', 'Mean Percentage', 'Smoking and Drug Experimentation by Peer Influence')

plot_grouped_bar(family_support_analysis, 'Family_Support_Level', 'Smoking_Prevalence', 'Drug_Experimentation',
                 'Family Support Level', 'Mean Percentage', 'Smoking and Drug Experimentation by Family Support')

plot_grouped_bar(school_program_analysis, 'School_Programs', 'Smoking_Prevalence', 'Drug_Experimentation',
                 'School Programs', 'Mean Percentage', 'Smoking and Drug Experimentation by School Programs')

plot_grouped_bar(education_analysis, 'Substance_Education', 'Smoking_Prevalence', 'Drug_Experimentation',
                 'Substance Education', 'Mean Percentage', 'Smoking and Drug Experimentation by Substance Education')

# Visualization of 4.5.2
# Age Group Analysis
plt.figure(figsize=(10, 6))
plt.plot(age_group_analysis['Age_Group'], age_group_analysis['Smoking_Prevalence'], marker='o', label='Smoking Prevalence')
plt.plot(age_group_analysis['Age_Group'], age_group_analysis['Drug_Experimentation'], marker='o', label='Drug Experimentation')
plt.legend()
plt.xlabel('Age Group')
plt.ylabel('Mean Percentage')
plt.title('Smoking and Drug Experimentation by Age Group')
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()

# Gender Analysis
plt.figure(figsize=(10, 6))
plt.plot(gender_analysis['Gender'], gender_analysis['Smoking_Prevalence'], marker='o', label='Smoking Prevalence')
plt.plot(gender_analysis['Gender'], gender_analysis['Drug_Experimentation'], marker='o', label='Drug Experimentation')
plt.legend()
plt.xlabel('Gender')
plt.ylabel('Mean Percentage')
plt.title('Smoking and Drug Experimentation by Gender')
plt.tight_layout()
plt.show()

# Socioeconomic Status Analysis
plt.figure(figsize=(10, 6))
plt.plot(socioecon_analysis['Socioeconomic_Status'], socioecon_analysis['Smoking_Prevalence'], marker='o', label='Smoking Prevalence')
plt.plot(socioecon_analysis['Socioeconomic_Status'], socioecon_analysis['Drug_Experimentation'], marker='o', label='Drug Experimentation')
plt.legend()
plt.xlabel('Socioeconomic Status')
plt.ylabel('Mean Percentage')
plt.title('Smoking and Drug Experimentation by Socioeconomic Status')
plt.tight_layout()
plt.show()

# Peer Influence Analysis
plt.figure(figsize=(10, 6))
plt.plot(peer_influence_analysis['Peer_Influence_Level'], peer_influence_analysis['Smoking_Prevalence'], marker='o', label='Smoking Prevalence')
plt.plot(peer_influence_analysis['Peer_Influence_Level'], peer_influence_analysis['Drug_Experimentation'], marker='o', label='Drug Experimentation')
plt.legend()
plt.xlabel('Peer Influence Level')
plt.ylabel('Mean Percentage')
plt.title('Smoking and Drug Experimentation by Peer Influence')
plt.tight_layout()
plt.show()

# Family Support Analysis
plt.figure(figsize=(10, 6))
plt.plot(family_support_analysis['Family_Support_Level'], family_support_analysis['Smoking_Prevalence'], marker='o', label='Smoking Prevalence')
plt.plot(family_support_analysis['Family_Support_Level'], family_support_analysis['Drug_Experimentation'], marker='o', label='Drug Experimentation')
plt.legend()
plt.xlabel('Family Support Level')
plt.ylabel('Mean Percentage')
plt.title('Smoking and Drug Experimentation by Family Support')
plt.tight_layout()
plt.show()

# School Programs Analysis
plt.figure(figsize=(10, 6))
plt.plot(school_program_analysis['School_Programs'], school_program_analysis['Smoking_Prevalence'], marker='o', label='Smoking Prevalence')
plt.plot(school_program_analysis['School_Programs'], school_program_analysis['Drug_Experimentation'], marker='o', label='Drug Experimentation')
plt.legend()
plt.xlabel('School Programs')
plt.ylabel('Mean Percentage')
plt.title('Smoking and Drug Experimentation by School Programs')
plt.tight_layout()
plt.show()

# Substance Education Analysis
plt.figure(figsize=(10, 6))
plt.plot(education_analysis['Substance_Education'], education_analysis['Smoking_Prevalence'], marker='o', label='Smoking Prevalence')
plt.plot(education_analysis['Substance_Education'], education_analysis['Drug_Experimentation'], marker='o', label='Drug Experimentation')
plt.legend()
plt.xlabel('Substance Education')
plt.ylabel('Mean Percentage')
plt.title('Smoking and Drug Experimentation by Substance Education')
plt.tight_layout()
plt.show()

"""# **5. Diagnostic Analytics**"""

# 5.1 Perform correlation analysis to assess relationships between numerical variables.
# Filter Numerical Columns for Correlation Analysis
numerical_data = df.select_dtypes(include=['float64', 'int64'])

# Calculate Correlation Matrix
print("\nCorrelation Matrix for Numerical Variables:")
correlation_matrix = numerical_data.corr()
print(correlation_matrix)

# Visualization of 5.1 (Correlations Heatmap)
import seaborn as sns
import matplotlib.pyplot as plt

plt.figure(figsize=(14, 12))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=".4f")
plt.title("Correlation Heatmap")
plt.show()

"""Review correlations to identify:

Strong positive correlations (close to +1).

Strong negative correlations (close to -1).

Weak or no correlation (close to 0).
"""

# 5.2 Use cross-tabulation to examine relationships between categorical variables
# Cross-Tabulation
import pandas as pd
from scipy.stats import chi2_contingency
categories_to_examine = [
    ('Gender', 'Socioeconomic_Status'),
    ('Gender', 'School_Programs'),
    ('Socioeconomic_Status', 'Substance_Education'),
    ('School_Programs', 'Substance_Education'),
    ('Gender', 'Substance_Education')
]

# Iterate through the category pairs and perform cross-tabulation and chi-square test
for var1, var2 in categories_to_examine:
    cross_tab = pd.crosstab(df[var1], df[var2])
    chi2, p, dof, expected = chi2_contingency(cross_tab)
    print(f"\nCross-tabulation between {var1} and {var2}:")
    print(cross_tab)
    print(f"Chi-Square: {chi2:.4f}, P-Value: {p:.4f}")

"""Cross-Tabulation: Compared Gender with School_Programs. The counts for "Yes" and "No" are fairly balanced across genders, which could imply no strong gender-based bias in participation in school programs.

T-Test: T-statistic: A very small value (-0.098), indicating minimal difference in smoking prevalence between males and females. P-value: 0.921, which is much greater than 0.05. This indicates that there is no statistically significant difference in smoking prevalence between males and females.
"""

# 5.3.1 Conduct regression analysis to investigate dependencies between variables(Smoking)
import pandas as pd
import statsmodels.api as sm
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, r2_score
import matplotlib.pyplot as plt

df2 = df.copy()

# Identify categorical columns
categorical_columns = df2.select_dtypes(include=['object']).columns

# Convert categorical variables to numerical
for col in categorical_columns:
    df2[col] = df2[col].astype('category').cat.codes

# Save the transformed dataset to a new CSV file
df2.to_csv('/drive/MyDrive/40 Final Project/Final Data/data2.csv', index=False)

# Define dependent and independent variables for regression
X = df2.drop(columns=['Smoking_Prevalence'])
y = df2['Smoking_Prevalence']

# Split data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Add constant to the model
X_train = sm.add_constant(X_train)
X_test = sm.add_constant(X_test)

# Fit the regression model
model = sm.OLS(y_train, X_train).fit()

# Print the summary of the regression analysis
print(model.summary())

# Predictions and evaluation
y_pred = model.predict(X_test)

# Calculate evaluation metrics
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)
print(f"\nMSE: {mse:.2f}, R²: {r2:.2f}")

# Plot actual vs predicted values
plt.figure(figsize=(8, 6))
plt.scatter(y_test, y_pred, alpha=0.7, color='blue', label='Predicted vs Actual')
plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'k--', lw=2, color='red', label='Ideal Fit')
plt.title('Actual vs Predicted Smoking Prevalence')
plt.xlabel('Actual Values')
plt.ylabel('Predicted Values')
plt.legend()
plt.tight_layout()
plt.show()

# 5.4.1 Identify variables with the most significant impact （no variables have significant impact on smoke）
print("\nVariables with the Most Significant Impact (Based on P-Value):")
significant_vars = model.pvalues[model.pvalues < 0.05].sort_values()
print("Significant Variables:\n", significant_vars)

#Check for multicollinearity (VIF > 10: Multicollinearity is usually considered serious and may need to be addressed.)
from statsmodels.stats.outliers_influence import variance_inflation_factor

vif_data = pd.DataFrame()
vif_data['Variable'] = X.columns
vif_data['VIF'] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]
print(vif_data)

# Use correlation analysis to select variables
correlation_matrix = df2.corr()
print(correlation_matrix['Smoking_Prevalence'].sort_values(ascending=False))

# 5.3.2 Conduct regression analysis to investigate dependencies between variables(Drug)
import pandas as pd
import statsmodels.api as sm
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, r2_score
import matplotlib.pyplot as plt

# Convert non-numerical variables to numerical
df2 = df.copy()

# Identify categorical columns
categorical_columns = df2.select_dtypes(include=['object']).columns

# Convert categorical variables to numerical
for col in categorical_columns:
    df2[col] = df2[col].astype('category').cat.codes

# Save the transformed dataset to a new CSV file
df2.to_csv('/drive/MyDrive/40 Final Project/Final Data/data2.csv', index=False)

# Define dependent and independent variables for regression
X = df2.drop(columns=['Drug_Experimentation'])  # Change target variable
y = df2['Drug_Experimentation']  # Use Drug_Experimentation as the target variable

# Split data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Add constant to the model
X_train = sm.add_constant(X_train)
X_test = sm.add_constant(X_test)

# Fit the regression model
model = sm.OLS(y_train, X_train).fit()

# Print the summary of the regression analysis
print(model.summary())

# Predictions and evaluation
y_pred = model.predict(X_test)

# Calculate evaluation metrics
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)
print(f"\nMSE: {mse:.2f}, R²: {r2:.2f}")

# Plot actual vs predicted values
plt.figure(figsize=(8, 6))
plt.scatter(y_test, y_pred, alpha=0.7, color='blue', label='Predicted vs Actual')
plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'k--', lw=2, color='red', label='Ideal Fit')
plt.title('Actual vs Predicted Drug Experimentation')
plt.xlabel('Actual Values')
plt.ylabel('Predicted Values')
plt.legend()
plt.tight_layout()
plt.show()

# 5.4.2 Identify variables with the most significant impact
print("\nVariables with the Most Significant Impact (Based on P-Value):")
significant_vars = model.pvalues[model.pvalues < 0.05].sort_values()
print("Significant Variables:\n", significant_vars)

"""# **6. Predictive Analytics**
Goal: Predict whether a youth is likely to engage in Smoking Prevalence or Drug Experimentation based on demographic and behavioral factors.
"""

# 6.2 Split the data into training and testing sets for model validation
# Encode Categorical Variables
from sklearn.preprocessing import LabelEncoder

# Encode all object-type columns
label_encoders = {}
for col in df.select_dtypes(include=['object']).columns:
    le = LabelEncoder()
    df[col] = le.fit_transform(df[col])
    label_encoders[col] = le  # Save encoder for future reference

# Split the Dataset into Training and Testing Sets
from sklearn.model_selection import train_test_split

# Predict 'Smoking_Prevalence' as an example
X = df.drop(['Smoking_Prevalence'], axis=1)  # Features
y = df['Smoking_Prevalence']                 # Target variable

# Split data (70% training, 30% testing)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)
print(f"Training set: {X_train.shape}, Testing set: {X_test.shape}")

# Convert 'Smoking_Prevalence' to Categorical Labels
# Define thresholds for 'Low', 'Medium', and 'High' prevalence
def categorize_smoking(prevalence):
    if prevalence <= 20:
        return 'Low'
    elif prevalence <= 35:
        return 'Medium'
    else:
        return 'High'

df['Smoking_Prevalence_Category'] = df['Smoking_Prevalence'].apply(categorize_smoking)

# Update target variable
X = df.drop(['Smoking_Prevalence', 'Smoking_Prevalence_Category'], axis=1)  # Features
y = df['Smoking_Prevalence_Category']                                     # Target variable

# Split the data again
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)
print(f"Training set: {X_train.shape}, Testing set: {X_test.shape}")

# 6.3 & 6.4 Build and train three different predictive models (smoke)
# Logistic Regression
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score

logreg = LogisticRegression(max_iter=1000)
logreg.fit(X_train, y_train)
y_pred_logreg = logreg.predict(X_test)
logreg_acc = accuracy_score(y_test, y_pred_logreg)
print(f"Logistic Regression Accuracy: {logreg_acc:.2f}")

# Decision Tree
from sklearn.tree import DecisionTreeClassifier

dtree = DecisionTreeClassifier(random_state=42)
dtree.fit(X_train, y_train)
y_pred_dtree = dtree.predict(X_test)
dtree_acc = accuracy_score(y_test, y_pred_dtree)
print(f"Decision Tree Accuracy: {dtree_acc:.2f}")

# Random Forest
from sklearn.ensemble import RandomForestClassifier

rf = RandomForestClassifier(random_state=42, n_estimators=100)
rf.fit(X_train, y_train)
y_pred_rf = rf.predict(X_test)
rf_acc = accuracy_score(y_test, y_pred_rf)
print(f"Random Forest Accuracy: {rf_acc:.2f}")

# Visualization for 6.4 (Smoke)
import matplotlib.pyplot as plt

# Model names and corresponding accuracies
models = ['Logistic Regression', 'Decision Tree', 'Random Forest']
accuracies = [0.32, 0.33, 0.34]  # Replace with actual accuracy values from the output

# Plot the accuracies
plt.figure(figsize=(8, 5))
plt.plot(models, accuracies, marker='o', linestyle='-', color='blue', label='Accuracy')

# Add titles and labels
plt.title('Model Accuracy Comparison')
plt.xlabel('Models')
plt.ylabel('Accuracy')
plt.ylim(0, 1)  # Set y-axis limits to emphasize comparison
plt.grid(True, linestyle='--', alpha=0.6)
plt.legend()
plt.tight_layout()

# Show the chart
plt.show()

"""Improve the models"""

# Logistic Regression with increased max_iter
logreg = LogisticRegression(max_iter=5000)
logreg.fit(X_train, y_train)
y_pred_logreg = logreg.predict(X_test)
logreg_acc = accuracy_score(y_test, y_pred_logreg)
print(f"Logistic Regression Accuracy (with increased max_iter): {logreg_acc:.2f}")

from sklearn.preprocessing import StandardScaler

# Scale features
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Logistic Regression with scaled features
logreg = LogisticRegression(max_iter=1000)
logreg.fit(X_train_scaled, y_train)
y_pred_logreg_scaled = logreg.predict(X_test_scaled)
logreg_acc_scaled = accuracy_score(y_test, y_pred_logreg_scaled)
print(f"Logistic Regression Accuracy (with Scaling): {logreg_acc_scaled:.2f}")

from sklearn.model_selection import GridSearchCV

# Hyperparameter tuning for Random Forest
param_grid = {
    'n_estimators': [50, 100, 150],
    'max_depth': [10, 20, None],
    'min_samples_split': [2, 5, 10],
}
rf = RandomForestClassifier(random_state=42)
grid_search = GridSearchCV(rf, param_grid, cv=3, scoring='accuracy')
grid_search.fit(X_train, y_train)

print(f"Best Parameters: {grid_search.best_params_}")
best_rf = grid_search.best_estimator_
rf_acc_optimized = accuracy_score(y_test, best_rf.predict(X_test))
print(f"Optimized Random Forest Accuracy: {rf_acc_optimized:.2f}")

# Visualization of 6.4 (Smoke)
# Convert y to Pandas Series (if needed)
if not isinstance(y, pd.Series):
    y_series = pd.Series(y, name="Smoking Prevalence Category")
else:
    y_series = y

# Plot the distribution of Smoking Prevalence Categories
y_series.value_counts().plot(kind='bar', color='skyblue')
plt.title("Distribution of Smoking Prevalence Categories")
plt.xlabel("Category")
plt.ylabel("Frequency")
plt.xticks(rotation=0)
plt.show()

# Compare Model Accuracy
models = ['Logistic Regression', 'Decision Tree', 'Random Forest']
accuracies = [logreg_acc, dtree_acc, rf_acc]  # Use actual accuracy scores
plt.bar(models, accuracies, color=['blue', 'green', 'orange'])
plt.title("Model Accuracy Comparison")
plt.xlabel("Models")
plt.ylabel("Accuracy")

# Accuracy ranges from 0 to 1
plt.ylim(0, 1)
plt.show()

# Feature Importance from Random Forest
importances = best_rf.feature_importances_
feature_names = X.columns

# Sort and plot
sorted_indices = np.argsort(importances)[::-1]
plt.figure(figsize=(10, 6))
plt.bar(range(len(importances)), importances[sorted_indices], align='center', color='lightcoral')
plt.xticks(range(len(importances)), feature_names[sorted_indices], rotation=90)
plt.title("Feature Importance (Random Forest)")
plt.show()

# 6.3 & 6.4 Build and train three different predictive models(drug)
# Convert 'Drug_Experimentation' to Categorical Labels
# Define thresholds for 'Low', 'Medium', and 'High' experimentation rates
def categorize_drug(experimentation):
    if experimentation <= 15:
        return 'Low'
    elif experimentation <= 30:
        return 'Medium'
    else:
        return 'High'

# Apply the categorization function
df['Drug_Experimentation_Category'] = df['Drug_Experimentation'].apply(categorize_drug)

# Define Features and Target
# Drop the original Drug_Experimentation and new category from features
X = df.drop(columns=['Drug_Experimentation', 'Drug_Experimentation_Category'])
y = df['Drug_Experimentation_Category']

# Encode Categorical Variables
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split

# Encode target variable
label_encoder = LabelEncoder()
y = label_encoder.fit_transform(y)  # Encode 'Low', 'Medium', 'High' as 0, 1, 2

# Encode features (if any categorical columns exist)
label_encoders = {}
for col in X.select_dtypes(include=['object']).columns:
    le = LabelEncoder()
    X[col] = le.fit_transform(X[col])
    label_encoders[col] = le

# Split the Dataset
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)
print(f"Training set: {X_train.shape}, Testing set: {X_test.shape}")

# Train and Evaluate Models

# Logistic Regression
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score

logreg = LogisticRegression(max_iter=1000)
logreg.fit(X_train, y_train)
y_pred_logreg = logreg.predict(X_test)
logreg_acc = accuracy_score(y_test, y_pred_logreg)
print(f"Logistic Regression Accuracy: {logreg_acc:.2f}")

# Decision Tree
from sklearn.tree import DecisionTreeClassifier

dtree = DecisionTreeClassifier(random_state=42)
dtree.fit(X_train, y_train)
y_pred_dtree = dtree.predict(X_test)
dtree_acc = accuracy_score(y_test, y_pred_dtree)
print(f"Decision Tree Accuracy: {dtree_acc:.2f}")

# Random Forest
from sklearn.ensemble import RandomForestClassifier

rf = RandomForestClassifier(random_state=42, n_estimators=100)
rf.fit(X_train, y_train)
y_pred_rf = rf.predict(X_test)
rf_acc = accuracy_score(y_test, y_pred_rf)
print(f"Random Forest Accuracy: {rf_acc:.2f}")

# Visualization for 6.3 (Drug)
import matplotlib.pyplot as plt

# Model names and corresponding accuracies
models = ['Logistic Regression', 'Decision Tree', 'Random Forest']
accuracies = [logreg_acc, dtree_acc, rf_acc]  # Use the accuracy values from the code

# Plot the accuracies
plt.figure(figsize=(8, 5))
plt.plot(models, accuracies, marker='o', linestyle='-', color='blue', label='Accuracy')

# Add titles and labels
plt.title('Model Accuracy Comparison (Drug Experimentation)')
plt.xlabel('Models')
plt.ylabel('Accuracy')
plt.ylim(0, 1)  # Ensure y-axis is between 0 and 1 for better scaling
plt.grid(True, linestyle='--', alpha=0.6)
plt.tight_layout()

# Show the chart
plt.show()

"""Improve the models"""

# Logistic Regression with increased max_iter
logreg = LogisticRegression(max_iter=5000)
logreg.fit(X_train, y_train)
y_pred_logreg = logreg.predict(X_test)
logreg_acc = accuracy_score(y_test, y_pred_logreg)
print(f"Logistic Regression Accuracy (with increased max_iter): {logreg_acc:.2f}")

from sklearn.preprocessing import StandardScaler

# Scale features
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Logistic Regression with scaled features
logreg = LogisticRegression(max_iter=1000)
logreg.fit(X_train_scaled, y_train)
y_pred_logreg_scaled = logreg.predict(X_test_scaled)
logreg_acc_scaled = accuracy_score(y_test, y_pred_logreg_scaled)
print(f"Logistic Regression Accuracy (with Scaling): {logreg_acc_scaled:.2f}")

from sklearn.model_selection import GridSearchCV

# Hyperparameter tuning for Random Forest
param_grid = {
    'n_estimators': [50, 100, 150],
    'max_depth': [10, 20, None],
    'min_samples_split': [2, 5, 10],
}
rf = RandomForestClassifier(random_state=42)
grid_search = GridSearchCV(rf, param_grid, cv=3, scoring='accuracy')
grid_search.fit(X_train, y_train)

print(f"Best Parameters: {grid_search.best_params_}")
best_rf = grid_search.best_estimator_
rf_acc_optimized = accuracy_score(y_test, best_rf.predict(X_test))
print(f"Optimized Random Forest Accuracy: {rf_acc_optimized:.2f}")

# Visualization of 6.4 (Drug)
# Convert y to Pandas Series (if needed)
if not isinstance(y, pd.Series):
    y_series = pd.Series(y, name="Drug Experimentation Category")
else:
    y_series = y

# Plot the distribution of Drug Experimentation Categories
y_series.value_counts().plot(kind='bar', color='skyblue')
plt.title("Distribution of Drug Experimentation Categories")
plt.xlabel("Category")
plt.ylabel("Frequency")
plt.xticks(rotation=0)
plt.show()

# Compare Model Accuracy for Drug Experimentation Prediction
models = ['Logistic Regression', 'Decision Tree', 'Random Forest']
accuracies = [logreg_acc, dtree_acc, rf_acc]  # Use actual accuracy scores for drug prediction
plt.bar(models, accuracies, color=['blue', 'green', 'orange'])
plt.title("Model Accuracy Comparison (Drug Experimentation)")
plt.xlabel("Models")
plt.ylabel("Accuracy")

# Accuracy ranges from 0 to 1
plt.ylim(0, 1)
plt.show()

# Feature Importance from Random Forest
importances = best_rf.feature_importances_
feature_names = X.columns

# Sort and plot
sorted_indices = np.argsort(importances)[::-1]
plt.figure(figsize=(10, 6))
plt.bar(range(len(importances)), importances[sorted_indices], align='center', color='lightcoral')
plt.xticks(range(len(importances)), feature_names[sorted_indices], rotation=90)
plt.title("Feature Importance (Random Forest)")
plt.show()